{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "os.chdir(\"../\")  # 注意这个cell,只能运行一次，不然工作路径会往上跳多级（../）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.fcos import FCOSDetector\n",
    "from dataset.VOC_dataset import VOCDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估标准mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def collect_evaluate_data(data_loader):\n",
    "    # 标签数据的容器\n",
    "    gt_boxes = []\n",
    "    gt_classes = []\n",
    "    # 预测数据的容器\n",
    "    pred_boxes = []\n",
    "    pred_classes = []\n",
    "    pred_scores = []\n",
    "\n",
    "    # 往两类容器中填值\n",
    "    for _, (img, boxes, classes) in enumerate(tqdm(data_loader)):\n",
    "    # for img, boxes, classes in data_loader:\n",
    "        with torch.no_grad():\n",
    "            out = model(img.cuda())\n",
    "            pred_boxes.append(out[2][0].cpu().numpy())\n",
    "            pred_classes.append(out[1][0].cpu().numpy())\n",
    "            pred_scores.append(out[0][0].cpu().numpy())\n",
    "        gt_boxes.append(boxes[0].numpy())\n",
    "        gt_classes.append(classes[0].numpy())\n",
    "\n",
    "    return gt_boxes, gt_classes, pred_boxes, pred_classes, pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def sort_by_score(pred_boxes, pred_labels, pred_scores):\n",
    "    score_seq = [(-score).argsort() for index, score in enumerate(pred_scores)]\n",
    "    pred_boxes = [sample_boxes[mask] for sample_boxes, mask in zip(pred_boxes, score_seq)]\n",
    "    pred_labels = [sample_boxes[mask] for sample_boxes, mask in zip(pred_labels, score_seq)]\n",
    "    pred_scores = [sample_boxes[mask] for sample_boxes, mask in zip(pred_scores, score_seq)]\n",
    "    return pred_boxes, pred_labels, pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def iou_2d(cubes_a, cubes_b):\n",
    "    \"\"\"\n",
    "    numpy 计算IoU\n",
    "    :param cubes_a: [N,(x1,y1,x2,y2)]\n",
    "    :param cubes_b: [M,(x1,y1,x2,y2)]\n",
    "    :return:  IoU [N,M]\n",
    "    \"\"\"\n",
    "    # expands dim\n",
    "    cubes_a = np.expand_dims(cubes_a, axis=1)  # [N,1,4]\n",
    "    cubes_b = np.expand_dims(cubes_b, axis=0)  # [1,M,4]\n",
    "    overlap = np.maximum(0.0,\n",
    "                         np.minimum(cubes_a[..., 2:], cubes_b[..., 2:]) -\n",
    "                         np.maximum(cubes_a[..., :2], cubes_b[..., :2]))  # [N,M,(w,h)]\n",
    "\n",
    "    # overlap\n",
    "    overlap = np.prod(overlap, axis=-1)  # [N,M]\n",
    "\n",
    "    # compute area\n",
    "    area_a = np.prod(cubes_a[..., 2:] - cubes_a[..., :2], axis=-1)\n",
    "    area_b = np.prod(cubes_b[..., 2:] - cubes_b[..., :2], axis=-1)\n",
    "\n",
    "    # compute iou\n",
    "    iou = overlap / (area_a + area_b - overlap)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def _compute_ap(recall, precision):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "    # Arguments\n",
    "        recall:    The recall curve (list).\n",
    "        precision: The precision curve (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "    # correct AP calculation\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], recall, [1.]))\n",
    "    mpre = np.concatenate(([0.], precision, [0.]))\n",
    "\n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def eval_ap_2d(gt_boxes, gt_labels, pred_boxes, pred_labels, pred_scores, iou_thread, num_cls):\n",
    "    \"\"\"\n",
    "    :param gt_boxes: list of 2d array,shape[(a,(x1,y1,x2,y2)),(b,(x1,y1,x2,y2))...]\n",
    "    :param gt_labels: list of 1d array,shape[(a),(b)...],value is sparse label index\n",
    "    :param pred_boxes: list of 2d array, shape[(m,(x1,y1,x2,y2)),(n,(x1,y1,x2,y2))...]\n",
    "    :param pred_labels: list of 1d array,shape[(m),(n)...],value is sparse label index\n",
    "    :param pred_scores: list of 1d array,shape[(m),(n)...]\n",
    "    :param iou_thread: eg. 0.5\n",
    "    :param num_cls: eg. 4, total number of class including background which is equal to 0\n",
    "    :return: a dict containing average precision for each cls\n",
    "    \"\"\"\n",
    "    all_ap = {}\n",
    "    for label in range(num_cls)[1:]:\n",
    "        # get samples with specific label\n",
    "        true_label_loc = [sample_labels == label for sample_labels in gt_labels]\n",
    "        gt_single_cls = [sample_boxes[mask] for sample_boxes, mask in zip(gt_boxes, true_label_loc)]\n",
    "\n",
    "        pred_label_loc = [sample_labels == label for sample_labels in pred_labels]\n",
    "        bbox_single_cls = [sample_boxes[mask] for sample_boxes, mask in zip(pred_boxes, pred_label_loc)]\n",
    "        scores_single_cls = [sample_scores[mask] for sample_scores, mask in zip(pred_scores, pred_label_loc)]\n",
    "\n",
    "        fp = np.zeros((0,))\n",
    "        tp = np.zeros((0,))\n",
    "        scores = np.zeros((0,))\n",
    "        total_gts = 0\n",
    "        # loop for each sample\n",
    "        for sample_gts, sample_pred_box, sample_scores in zip(gt_single_cls, bbox_single_cls, scores_single_cls):\n",
    "            total_gts = total_gts + len(sample_gts)\n",
    "            assigned_gt = []  # one gt can only be assigned to one predicted bbox\n",
    "            # loop for each predicted bbox\n",
    "            for index in range(len(sample_pred_box)):\n",
    "                scores = np.append(scores, sample_scores[index])\n",
    "                if len(sample_gts) == 0:  # if no gts found for the predicted bbox, assign the bbox to fp\n",
    "                    fp = np.append(fp, 1)\n",
    "                    tp = np.append(tp, 0)\n",
    "                    continue\n",
    "                pred_box = np.expand_dims(sample_pred_box[index], axis=0)\n",
    "                iou = iou_2d(sample_gts, pred_box)\n",
    "                gt_for_box = np.argmax(iou, axis=0)\n",
    "                max_overlap = iou[gt_for_box, 0]\n",
    "                if max_overlap >= iou_thread and gt_for_box not in assigned_gt:\n",
    "                    fp = np.append(fp, 0)\n",
    "                    tp = np.append(tp, 1)\n",
    "                    assigned_gt.append(gt_for_box)\n",
    "                else:\n",
    "                    fp = np.append(fp, 1)\n",
    "                    tp = np.append(tp, 0)\n",
    "        # sort by score\n",
    "        indices = np.argsort(-scores)\n",
    "        fp = fp[indices]\n",
    "        tp = tp[indices]\n",
    "        # compute cumulative false positives and true positives\n",
    "        fp = np.cumsum(fp)\n",
    "        tp = np.cumsum(tp)\n",
    "        # compute recall and precision\n",
    "        recall = tp / total_gts\n",
    "        precision = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
    "        ap = _compute_ap(recall, precision)\n",
    "        all_ap[label] = ap\n",
    "        # print(recall, precision)\n",
    "    return all_ap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据和模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO=====>voc dataset init finished  ! !\n",
      "INFO===>eval dataset has 10 imgs\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = VOCDataset(root_dir='./notebooks/dataset/VOCdevkit/VOC2012', resize_size=[800, 1333],\n",
    "                          split='trainval_demoData', use_difficult=False, is_train=False, augment=None)\n",
    "print(\"INFO===>eval dataset has %d imgs\" % len(eval_dataset))\n",
    "eval_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=1, shuffle=False,\n",
    "                                          collate_fn=eval_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO===>success frozen BN\n",
      "INFO===>success frozen backbone stage1\n",
      "===>success loading model\n"
     ]
    }
   ],
   "source": [
    "model = FCOSDetector(mode=\"inference\")\n",
    "model = torch.nn.DataParallel(model) \n",
    "model.load_state_dict(torch.load(\"./checkpoint/voc_77.8.pth\", map_location=torch.device('cpu')))\n",
    "model = model.cuda().eval()\n",
    "print(\"===>success loading model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:07<00:00,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# 收集数据\n",
    "gt_boxes, gt_classes, pred_boxes, pred_classes, pred_scores = collect_evaluate_data(eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[ 70.4     ,  21.333334, 953.60004 , 622.93335 ]], dtype=float32),\n",
       "  array([[ 108.10811 ,   24.024025, 1198.7988  ,  797.5976  ],\n",
       "         [ 146.54654 ,  454.05405 ,  196.997   ,  581.3814  ]],\n",
       "        dtype=float32),\n",
       "  array([[  0.     , 488.53336, 910.9334 , 622.93335]], dtype=float32),\n",
       "  array([[ 94.117645, 155.65611 , 850.6787  , 758.37103 ],\n",
       "         [284.1629  ,  77.82806 , 521.26697 , 300.45248 ]], dtype=float32),\n",
       "  array([[ 460.80002,  341.33334,  625.0667 ,  469.33334],\n",
       "         [ 989.8667 ,  354.13336, 1064.5334 ,  462.93335]], dtype=float32),\n",
       "  array([[  27.733335,  313.6     , 1011.2001  ,  612.2667  ]],\n",
       "        dtype=float32),\n",
       "  array([[ 12.0120125,   0.       , 751.95197  , 627.02704  ],\n",
       "         [ 93.693695 , 230.63063  , 288.2883   , 984.985    ],\n",
       "         [326.7267   ,  84.08408  , 403.6036   , 259.45947  ],\n",
       "         [430.03003  ,  84.08408  , 516.51654  , 247.44745  ],\n",
       "         [228.22823  ,  91.29129  , 293.09308  , 245.04504  ]],\n",
       "        dtype=float32),\n",
       "  array([[258.13336 ,  12.800001, 791.46674 , 797.8667  ],\n",
       "         [448.00003 , 311.46667 , 691.2     , 541.8667  ]], dtype=float32),\n",
       "  array([[  17.066668,   66.13334 , 1064.5334  ,  797.8667  ]],\n",
       "        dtype=float32),\n",
       "  array([[334.93335, 422.40002, 490.6667 , 509.8667 ],\n",
       "         [ 98.13334, 428.80002, 198.40001, 505.60004]], dtype=float32)],\n",
       " [array([20], dtype=int64),\n",
       "  array([19, 15], dtype=int64),\n",
       "  array([4], dtype=int64),\n",
       "  array([13, 15], dtype=int64),\n",
       "  array([10, 10], dtype=int64),\n",
       "  array([1], dtype=int64),\n",
       "  array([20,  5, 15, 15, 15], dtype=int64),\n",
       "  array([15, 12], dtype=int64),\n",
       "  array([7], dtype=int64),\n",
       "  array([7, 7], dtype=int64)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_boxes, gt_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238, 4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_boxes[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_classes[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_scores[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调整预测数据\n",
    "pred_boxes, pred_classes, pred_scores = sort_by_score(pred_boxes, pred_classes, pred_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\conda\\envs\\torch1.1\\lib\\site-packages\\ipykernel_launcher.py:56: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# 拿到数据后开始计算AP\n",
    "all_AP = eval_ap_2d(gt_boxes, gt_classes, pred_boxes, pred_classes, pred_scores, \n",
    "                    0.5, len(eval_dataset.CLASSES_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code is trying to \"divide by zero\" or \"divide by NaN\". If you are aware of that and don't want it to bother you, then you can try:\n",
    ">np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all classes AP=====>\n",
      "\n",
      "ap for aeroplane is 1.0\n",
      "ap for bicycle is nan\n",
      "ap for bird is nan\n",
      "ap for boat is 1.0\n",
      "ap for bottle is 1.0\n",
      "ap for bus is nan\n",
      "ap for car is 1.0\n",
      "ap for cat is nan\n",
      "ap for chair is nan\n",
      "ap for cow is 1.0\n",
      "ap for diningtable is nan\n",
      "ap for dog is 1.0\n",
      "ap for horse is 1.0\n",
      "ap for motorbike is nan\n",
      "ap for person is 0.8472222222222223\n",
      "ap for pottedplant is nan\n",
      "ap for sheep is nan\n",
      "ap for sofa is nan\n",
      "ap for train is 1.0\n",
      "ap for tvmonitor is 1.0\n",
      "mAP=====>nan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 计算每一类的AP\n",
    "print(\"all classes AP=====>\\n\")\n",
    "for key, value in all_AP.items():\n",
    "    print('ap for {} is {}'.format(eval_dataset.id2name[int(key)], value))\n",
    "    \n",
    "# 计算mAP    \n",
    "mAP = 0.\n",
    "for class_id, class_mAP in all_AP.items():\n",
    "    mAP += float(class_mAP)\n",
    "    \n",
    "mAP /= (len(eval_dataset.CLASSES_NAME) - 1)\n",
    "print(\"mAP=====>%.3f\\n\" % mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all classes AP=====>\n",
      "\n",
      "ap for aeroplane is 0.8467718402495026\n",
      "ap for bicycle is 0.8574527632509155\n",
      "ap for bird is 0.8424980411647369\n",
      "ap for boat is 0.6914078119084456\n",
      "ap for bottle is 0.7095268316636022\n",
      "ap for bus is 0.8592892408461792\n",
      "ap for car is 0.902261583538996\n",
      "ap for cat is 0.923486914964034\n",
      "ap for chair is 0.5606274429578793\n",
      "ap for cow is 0.8475735368616513\n",
      "ap for diningtable is 0.662367861880638\n",
      "ap for dog is 0.8932658016045054\n",
      "ap for horse is 0.848708053056753\n",
      "ap for motorbike is 0.8132639327493589\n",
      "ap for person is 0.8577320539239368\n",
      "ap for pottedplant is 0.4828017808821626\n",
      "ap for sheep is 0.8291090740890208\n",
      "ap for sofa is 0.7029149826989491\n",
      "ap for train is 0.8702907780568689\n",
      "ap for tvmonitor is 0.8216938510961964\n",
      "mAP=====>0.791\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# voc2007 test\n",
    "eval_dataset = VOCDataset(root_dir='../datasets/VOCdevkit/VOC2007', resize_size=[800, 1333],\n",
    "                          split='test', use_difficult=False, is_train=False, augment=None)\n",
    "print(\"INFO===>eval dataset has %d imgs\" % len(eval_dataset))\n",
    "eval_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=1, shuffle=False,\n",
    "                                          collate_fn=eval_dataset.collate_fn)\n",
    "\n",
    "model = FCOSDetector(mode=\"inference\")\n",
    "model = torch.nn.DataParallel(model) \n",
    "model.load_state_dict(torch.load(\"./checkpoint/voc_77.8.pth\", map_location=torch.device('cpu')))\n",
    "model = model.cuda().eval()\n",
    "print(\"===>success loading model\")\n",
    "\n",
    "\n",
    "# 标签数据的容器\n",
    "gt_boxes = []\n",
    "gt_classes = []\n",
    "\n",
    "# 预测数据的容器\n",
    "pred_boxes = []\n",
    "pred_classes = []\n",
    "pred_scores = []\n",
    "\n",
    "# 往两类容器中填值\n",
    "num = 0\n",
    "for img, boxes, classes in eval_loader:\n",
    "    with torch.no_grad():\n",
    "        out = model(img.cuda())\n",
    "        pred_boxes.append(out[2][0].cpu().numpy())\n",
    "        pred_classes.append(out[1][0].cpu().numpy())\n",
    "        pred_scores.append(out[0][0].cpu().numpy())\n",
    "    gt_boxes.append(boxes[0].numpy())\n",
    "    gt_classes.append(classes[0].numpy())\n",
    "    num += 1\n",
    "    print(num, end='\\r')\n",
    "    \n",
    "# 拿到数据后开始计算AP\n",
    "pred_boxes, pred_classes, pred_scores = sort_by_score(pred_boxes, pred_classes, pred_scores)\n",
    "\n",
    "all_AP = eval_ap_2d(gt_boxes, gt_classes, pred_boxes, pred_classes, pred_scores, 0.5,\n",
    "                    len(eval_dataset.CLASSES_NAME))\n",
    "\n",
    "\n",
    "# 计算每一类的AP\n",
    "print(\"all classes AP=====>\\n\")\n",
    "for key, value in all_AP.items():\n",
    "    print('ap for {} is {}'.format(eval_dataset.id2name[int(key)], value))\n",
    "    \n",
    "# 计算mAP    \n",
    "mAP = 0.\n",
    "for class_id, class_mAP in all_AP.items():\n",
    "    mAP += float(class_mAP)\n",
    "    \n",
    "mAP /= (len(eval_dataset.CLASSES_NAME) - 1)\n",
    "print(\"mAP=====>%.3f\\n\" % mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "torch1.1",
   "language": "python",
   "name": "torch1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
