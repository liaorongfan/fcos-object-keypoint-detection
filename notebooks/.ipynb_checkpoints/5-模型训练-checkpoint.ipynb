{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "os.chdir('../')  # 更改notebook的工作路径到上一级目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.VOC_dataset import VOCDataset\n",
    "from dataset.augment import Transforms\n",
    "import torch.nn as nn\n",
    "from model.backbone.resnet import resnet50\n",
    "from model.fcos import FCOS\n",
    "from model.loss import GenTargets, LOSS, coords_fmap2orig\n",
    "from model.fpn_neck import FPN\n",
    "from model.config import DefaultConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--epochs\", type=int, default=30, help=\"number of epochs\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=1, help=\"size of each image batch\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=0, help=\"number of cpu threads to use during batch generation\")\n",
    "parser.add_argument(\"--n_gpu\", type=str, default='0,1', help=\"number of cpu threads to use during batch generation\")\n",
    "opt = parser.parse_args([])  # notebook 中运行的时候要加 parser.parse_args() 的参数要加[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU环境设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = opt.n_gpu\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "np.random.seed(0)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO=====>voc dataset init finished  ! !\n",
      "total_images : 5011\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = opt.batch_size\n",
    "\n",
    "transform = Transforms()\n",
    "train_dataset = VOCDataset(root_dir='../datasets/VOCdevkit/VOC2007', resize_size=[800, 1333],\n",
    "                           split='trainval', use_difficult=False, is_train=True, augment=transform)\n",
    "\n",
    "# WARMPUP_STEPS_RATIO = 0.12\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                           collate_fn=train_dataset.collate_fn,\n",
    "                                           num_workers=opt.n_cpu, worker_init_fn=np.random.seed(0))\n",
    "\n",
    "print(\"total_images : {}\".format(len(train_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCOSDetector(nn.Module):\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "        if config is None:\n",
    "            config = DefaultConfig\n",
    "\n",
    "        self.fcos_body = FCOS(config=config)\n",
    "        self.target_layer = GenTargets(strides=config.strides, limit_range=config.limit_range)\n",
    "        self.loss_layer = LOSS()\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        FCOS网络\n",
    "        :param inputs:\n",
    "                [training] list  batch_images,batch_boxes,batch_classes\n",
    "        :return:\n",
    "                [training] losses\n",
    "        \"\"\"\n",
    "        batch_imgs, batch_boxes, batch_classes = inputs\n",
    "        # 模型输出\n",
    "        out = self.fcos_body(batch_imgs)\n",
    "        # 编码标签信息\n",
    "        targets = self.target_layer([out, batch_boxes, batch_classes])\n",
    "        # 计算标签和预测信息间的损失\n",
    "        losses = self.loss_layer([out, targets])\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO===>success frozen BN\n",
      "INFO===>success frozen backbone stage1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FCOSDetector(\n",
       "  (fcos_body): FCOS(\n",
       "    (backbone): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "    )\n",
       "    (fpn): FPN(\n",
       "      (prj_5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (prj_4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (prj_3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv_5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_out6): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv_out7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (head): ClsCntRegHead(\n",
       "      (cls_conv): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (2): ReLU(inplace)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (5): ReLU(inplace)\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (8): ReLU(inplace)\n",
       "        (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (10): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (11): ReLU(inplace)\n",
       "      )\n",
       "      (reg_conv): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (2): ReLU(inplace)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (5): ReLU(inplace)\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (8): ReLU(inplace)\n",
       "        (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (10): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (11): ReLU(inplace)\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cnt_logits): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (reg_pred): Conv2d(256, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (scale_exp): ModuleList(\n",
       "        (0): ScaleExp()\n",
       "        (1): ScaleExp()\n",
       "        (2): ScaleExp()\n",
       "        (3): ScaleExp()\n",
       "        (4): ScaleExp()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (target_layer): GenTargets()\n",
       "  (loss_layer): LOSS()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FCOSDetector().cuda()\n",
    "# model = torch.nn.DataParallel(model)  # 多gpu时使用\n",
    "model.train()  # 设置为训练模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化器设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=2e-3, momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     9
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_steps:1 epoch:1 steps:1/5011 cls_loss:1.2560 cnt_loss:0.7801 reg_loss:0.9997 cost_time:14171ms lr=3.9920e-06 total_loss:3.0358\n",
      "global_steps:2 epoch:1 steps:2/5011 cls_loss:1.1750 cnt_loss:0.7514 reg_loss:0.9999 cost_time:534ms lr=7.9840e-06 total_loss:2.9263\n",
      "global_steps:3 epoch:1 steps:3/5011 cls_loss:1.2949 cnt_loss:0.7793 reg_loss:0.9997 cost_time:397ms lr=1.1976e-05 total_loss:3.0740\n",
      "global_steps:4 epoch:1 steps:4/5011 cls_loss:1.3756 cnt_loss:0.9157 reg_loss:1.0000 cost_time:399ms lr=1.5968e-05 total_loss:3.2913\n",
      "global_steps:5 epoch:1 steps:5/5011 cls_loss:1.0905 cnt_loss:0.7375 reg_loss:0.9995 cost_time:398ms lr=1.9960e-05 total_loss:2.8276\n",
      "global_steps:6 epoch:1 steps:6/5011 cls_loss:1.1989 cnt_loss:0.8049 reg_loss:1.0000 cost_time:428ms lr=2.3952e-05 total_loss:3.0039\n",
      "global_steps:7 epoch:1 steps:7/5011 cls_loss:1.0979 cnt_loss:0.7145 reg_loss:1.0000 cost_time:390ms lr=2.7944e-05 total_loss:2.8123\n",
      "global_steps:8 epoch:1 steps:8/5011 cls_loss:2.7303 cnt_loss:0.0000 reg_loss:0.0000 cost_time:392ms lr=3.1936e-05 total_loss:2.7303\n",
      "global_steps:9 epoch:1 steps:9/5011 cls_loss:1.2188 cnt_loss:0.7174 reg_loss:1.0000 cost_time:395ms lr=3.5928e-05 total_loss:2.9362\n",
      "global_steps:10 epoch:1 steps:10/5011 cls_loss:1.2737 cnt_loss:0.7154 reg_loss:0.9998 cost_time:400ms lr=3.9920e-05 total_loss:2.9889\n",
      "global_steps:11 epoch:1 steps:11/5011 cls_loss:1.4557 cnt_loss:0.6942 reg_loss:1.0000 cost_time:348ms lr=4.3912e-05 total_loss:3.1498\n",
      "global_steps:12 epoch:1 steps:12/5011 cls_loss:1.1691 cnt_loss:0.6975 reg_loss:0.9999 cost_time:490ms lr=4.7904e-05 total_loss:2.8666\n",
      "global_steps:13 epoch:1 steps:13/5011 cls_loss:1.2077 cnt_loss:0.6686 reg_loss:0.9998 cost_time:415ms lr=5.1896e-05 total_loss:2.8761\n",
      "global_steps:14 epoch:1 steps:14/5011 cls_loss:1.1993 cnt_loss:0.6738 reg_loss:1.0000 cost_time:380ms lr=5.5888e-05 total_loss:2.8732\n",
      "global_steps:15 epoch:1 steps:15/5011 cls_loss:1.1539 cnt_loss:0.6075 reg_loss:0.9999 cost_time:412ms lr=5.9880e-05 total_loss:2.7614\n",
      "global_steps:16 epoch:1 steps:16/5011 cls_loss:1.1663 cnt_loss:0.6281 reg_loss:0.9999 cost_time:398ms lr=6.3872e-05 total_loss:2.7943\n",
      "global_steps:17 epoch:1 steps:17/5011 cls_loss:1.1127 cnt_loss:0.6198 reg_loss:1.0000 cost_time:423ms lr=6.7864e-05 total_loss:2.7325\n",
      "global_steps:18 epoch:1 steps:18/5011 cls_loss:1.2072 cnt_loss:0.6250 reg_loss:1.0000 cost_time:361ms lr=7.1856e-05 total_loss:2.8322\n",
      "global_steps:19 epoch:1 steps:19/5011 cls_loss:1.1957 cnt_loss:0.6740 reg_loss:0.9999 cost_time:348ms lr=7.5848e-05 total_loss:2.8696\n",
      "global_steps:20 epoch:1 steps:20/5011 cls_loss:1.2434 cnt_loss:0.7126 reg_loss:1.0000 cost_time:336ms lr=7.9840e-05 total_loss:2.9559\n",
      "global_steps:21 epoch:1 steps:21/5011 cls_loss:1.1557 cnt_loss:0.6525 reg_loss:1.0000 cost_time:337ms lr=8.3832e-05 total_loss:2.8082\n",
      "global_steps:22 epoch:1 steps:22/5011 cls_loss:1.0313 cnt_loss:1.3009 reg_loss:0.9975 cost_time:396ms lr=8.7824e-05 total_loss:3.3296\n",
      "global_steps:23 epoch:1 steps:23/5011 cls_loss:1.1538 cnt_loss:0.5994 reg_loss:1.0000 cost_time:381ms lr=9.1816e-05 total_loss:2.7532\n",
      "global_steps:24 epoch:1 steps:24/5011 cls_loss:0.9770 cnt_loss:0.6365 reg_loss:1.0000 cost_time:362ms lr=9.5808e-05 total_loss:2.6134\n",
      "global_steps:25 epoch:1 steps:25/5011 cls_loss:2.1144 cnt_loss:0.0000 reg_loss:0.0000 cost_time:412ms lr=9.9800e-05 total_loss:2.1144\n",
      "global_steps:26 epoch:1 steps:26/5011 cls_loss:1.0423 cnt_loss:0.6545 reg_loss:0.9991 cost_time:402ms lr=1.0379e-04 total_loss:2.6959\n",
      "global_steps:27 epoch:1 steps:27/5011 cls_loss:1.1272 cnt_loss:0.6251 reg_loss:1.0000 cost_time:436ms lr=1.0778e-04 total_loss:2.7523\n",
      "global_steps:28 epoch:1 steps:28/5011 cls_loss:1.1401 cnt_loss:0.6326 reg_loss:1.0000 cost_time:346ms lr=1.1178e-04 total_loss:2.7727\n",
      "global_steps:29 epoch:1 steps:29/5011 cls_loss:1.2137 cnt_loss:0.6608 reg_loss:0.9999 cost_time:432ms lr=1.1577e-04 total_loss:2.8744\n",
      "global_steps:30 epoch:1 steps:30/5011 cls_loss:1.2177 cnt_loss:0.0000 reg_loss:0.0000 cost_time:383ms lr=1.1976e-04 total_loss:1.2177\n",
      "global_steps:31 epoch:1 steps:31/5011 cls_loss:1.4469 cnt_loss:0.6670 reg_loss:1.0000 cost_time:381ms lr=1.2375e-04 total_loss:3.1139\n",
      "global_steps:32 epoch:1 steps:32/5011 cls_loss:1.1998 cnt_loss:0.6517 reg_loss:1.0000 cost_time:356ms lr=1.2774e-04 total_loss:2.8514\n",
      "global_steps:33 epoch:1 steps:33/5011 cls_loss:1.1265 cnt_loss:0.6495 reg_loss:1.0000 cost_time:361ms lr=1.3174e-04 total_loss:2.7760\n",
      "global_steps:34 epoch:1 steps:34/5011 cls_loss:1.2868 cnt_loss:0.6270 reg_loss:0.9999 cost_time:408ms lr=1.3573e-04 total_loss:2.9137\n",
      "global_steps:35 epoch:1 steps:35/5011 cls_loss:1.5313 cnt_loss:0.6933 reg_loss:1.0000 cost_time:425ms lr=1.3972e-04 total_loss:3.2246\n",
      "global_steps:36 epoch:1 steps:36/5011 cls_loss:1.2487 cnt_loss:0.6375 reg_loss:1.0000 cost_time:344ms lr=1.4371e-04 total_loss:2.8861\n",
      "global_steps:37 epoch:1 steps:37/5011 cls_loss:1.1982 cnt_loss:0.6666 reg_loss:0.9998 cost_time:415ms lr=1.4770e-04 total_loss:2.8646\n",
      "global_steps:38 epoch:1 steps:38/5011 cls_loss:1.2123 cnt_loss:0.6799 reg_loss:1.0000 cost_time:424ms lr=1.5170e-04 total_loss:2.8922\n",
      "global_steps:39 epoch:1 steps:39/5011 cls_loss:1.1702 cnt_loss:0.7046 reg_loss:1.0000 cost_time:393ms lr=1.5569e-04 total_loss:2.8748\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-d48fa507ccd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;34m\"global_steps:%d epoch:%d steps:%d/%d cls_loss:%.4f cnt_loss:%.4f reg_loss:%.4f cost_time:%dms lr=%.4e total_loss:%.4f\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             (GLOBAL_STEPS, epoch + 1, epoch_step + 1, steps_per_epoch, losses[0].mean(), losses[1].mean(),\n\u001b[1;32m---> 56\u001b[1;33m              losses[2].mean(), cost_time, lr, loss.mean()))\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mGLOBAL_STEPS\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = opt.epochs\n",
    "steps_per_epoch = len(train_dataset) // BATCH_SIZE\n",
    "TOTAL_STEPS = steps_per_epoch * EPOCHS\n",
    "WARMPUP_STEPS = 501\n",
    "\n",
    "GLOBAL_STEPS = 1\n",
    "LR_INIT = 2e-3\n",
    "LR_END = 2e-5\n",
    "\n",
    "for epoch in range(EPOCHS):  # 分轮次，，，\n",
    "    for epoch_step, data in enumerate(train_loader):  # ，，，分批次 开始训练\n",
    "        \n",
    "        # ============================== 拿到批次数据 =========================\n",
    "        batch_imgs, batch_boxes, batch_classes = data\n",
    "        batch_imgs = batch_imgs.cuda()\n",
    "        batch_boxes = batch_boxes.cuda()\n",
    "        batch_classes = batch_classes.cuda()\n",
    "        # =====================================================================\n",
    "        \n",
    "        \n",
    "        # ================================ 学习率调整 =========================\n",
    "        if GLOBAL_STEPS < WARMPUP_STEPS:\n",
    "            lr = float(GLOBAL_STEPS / WARMPUP_STEPS * LR_INIT)\n",
    "            for param in optimizer.param_groups:\n",
    "                param['lr'] = lr\n",
    "        if GLOBAL_STEPS == 20001:\n",
    "            lr = LR_INIT * 0.1\n",
    "            for param in optimizer.param_groups:\n",
    "                param['lr'] = lr\n",
    "        if GLOBAL_STEPS == 27001:\n",
    "            lr = LR_INIT * 0.01\n",
    "            for param in optimizer.param_groups:\n",
    "                param['lr'] = lr       \n",
    "        # ===================================================================   \n",
    "        \n",
    "        \n",
    "        # ============================ 网络参数更新 =========================\n",
    "        start_time = time.time()\n",
    "        # 1 梯度清理\n",
    "        optimizer.zero_grad()\n",
    "        # 2 损失计算\n",
    "        losses = model([batch_imgs, batch_boxes, batch_classes])\n",
    "        loss = losses[-1]\n",
    "        loss.mean().backward()\n",
    "        # 3 梯度回传更新网络参数\n",
    "        optimizer.step()\n",
    "        # =================================================================\n",
    "        \n",
    "        \n",
    "        # ============================ 显示训练信息 =========================\n",
    "        end_time = time.time()\n",
    "        cost_time = int((end_time - start_time) * 1000)\n",
    "        print(\n",
    "            \"global_steps:%d epoch:%d steps:%d/%d cls_loss:%.4f cnt_loss:%.4f reg_loss:%.4f cost_time:%dms lr=%.4e total_loss:%.4f\" % \\\n",
    "            (GLOBAL_STEPS, epoch + 1, epoch_step + 1, steps_per_epoch, losses[0].mean(), losses[1].mean(),\n",
    "             losses[2].mean(), cost_time, lr, loss.mean()))\n",
    "\n",
    "        GLOBAL_STEPS += 1\n",
    "        # ==================================================================\n",
    "    torch.save(model.state_dict(),\n",
    "               \"./checkpoint/model_{}.pth\".format(epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "torch1.1",
   "language": "python",
   "name": "torch1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
