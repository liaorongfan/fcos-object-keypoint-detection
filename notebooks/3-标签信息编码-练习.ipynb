{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random \n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor & Feature map**\n",
    "\n",
    "Tensor:(h, w, c)=(2,5,3) ;\n",
    "\n",
    "Feature map:(b, h, w, c)=(3,2,4,5)\n",
    "\n",
    "<img src=\"imgs/3-axis_front.png\" width=\"400\" height=\"400\" align=\"left\">\n",
    "\n",
    "<img src=\"imgs/4-axis_block.png\" width=\"300\" height=\"400\" align=\"center\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 假设一张输入图片的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 一个标签框的情况\n",
    "# gt_boxes = np.array([[[10, 12, 48, 60]]]) \n",
    "# 一个gt_box对应的类别标签\n",
    "# classes = np.array([[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of gt_box in one img: 2\n",
      "gt_boxes: torch.Size([1, 2, 4]) torch.float32\n",
      "classes: torch.Size([1, 2]) torch.int32\n"
     ]
    }
   ],
   "source": [
    "# 当前图片中有两个标签框的情况，每个框由4个坐标表示，设定batch_size=1,\n",
    "gt_boxes = np.array([[[10, 12, 48, 60],\n",
    "                      [10, 20, 40, 50]]]) \n",
    "# 这两个gt_box各自对应的类别\n",
    "classes = np.array([[2,4]])  # （batch_size, num_of_gtbox)\n",
    "m = gt_boxes.shape[1]; print('num of gt_box in one img:',m)  # 标签框个数\n",
    "\n",
    "# 转变numpy的ndarray的数据类型，到torch的tensor数据类型，\n",
    "gt_boxes = torch.from_numpy(gt_boxes)\n",
    "gt_boxes = gt_boxes.type(torch.float32)  # 转换张量的数值类型，方便后续计算\n",
    "classes = torch.from_numpy(classes)\n",
    "\n",
    "print('gt_boxes:', gt_boxes.shape, gt_boxes.dtype)\n",
    "print('classes:', classes.shape, classes.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这张图片输入模型后，模型输出的预测值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**假设网络有三层输出(h,w)大小依次为 （16，16） （8，8） （4，4）**\n",
    "\n",
    "batch_size = 1；  num_of_class = 5（即图中classification的通道数C=5）\n",
    "<img src=\"imgs/head_demo.png\" width=\"500\" height=\"400\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有的分类层\n",
    "cls_logits_all = [random.rand(1, 5, 16, 16),  # 第一层 \n",
    "                 random.rand(1, 5, 8, 8),     # 第二层\n",
    "                 random.rand(1, 5, 4, 4),]    # 第三层\n",
    "# 所有的中心度层\n",
    "cnt_logits_all = [random.rand(1, 1, 16, 16),  # 第一层 \n",
    "                 random.rand(1, 1, 8, 8),     # 第二层\n",
    "                 random.rand(1, 1, 4, 4),]    # 第三层\n",
    "# 所有的回归层\n",
    "reg_preds_all = [random.rand(1, 4, 16, 16),   # 第一层 \n",
    "                 random.rand(1, 4, 8, 8),     # 第二层\n",
    "                 random.rand(1, 4, 4, 4),]    # 第三层\n",
    "\n",
    "fcos_out = [cls_logits_all, cnt_logits_all, reg_preds_all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**不同层的网络输出**\n",
    "\n",
    "每一层都包括 分类输出、中心度输出和坐标偏置输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_0_out = [cls_logits_all[0], cnt_logits_all[0], reg_preds_all[0]]  # 第一层\n",
    "level_1_out = [cls_logits_all[1], cnt_logits_all[1], reg_preds_all[1]]  # 第二层\n",
    "level_2_out = [cls_logits_all[2], cnt_logits_all[2], reg_preds_all[2]]  # 第三层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征图信息编码（映射/转换）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征图上格点坐标与原图坐标的映射(对应)关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 1\n",
      "feature_map_hw: torch.Size([4, 4])\n",
      "class_num: 5\n",
      "torch.Size([1, 4, 4, 5]) torch.float64\n"
     ]
    }
   ],
   "source": [
    "# 以第三层的运算为例 看一下第三个输出层中分类分支特征图的数据形式\n",
    "cls_logits, cnt_logits, reg_preds = cls_logits_all[2], cnt_logits_all[2], reg_preds_all[2]\n",
    "\n",
    "cls_logits = torch.from_numpy(cls_logits)  # 将numpy格式数据转换成tensor\n",
    "cls_logits = cls_logits.permute(0, 2, 3, 1)  # [batch_size,h,w,class_num]\n",
    "\n",
    "batch_size = cls_logits.shape[0] ;print('batch_size:', batch_size)\n",
    "hw = cls_logits.shape[1:3]; print('feature_map_hw:', hw)  # 图像的高宽\n",
    "class_num = cls_logits.shape[3] ;print('class_num:', class_num)\n",
    "print(cls_logits.shape, cls_logits.dtype)  # 特征图上每一个grid是一个5维的向量  用来表示当前点的类别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/shape.png\" width=\"300\" height=\"400\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.,  8.],\n",
       "        [24.,  8.],\n",
       "        [40.,  8.],\n",
       "        [56.,  8.],\n",
       "        [ 8., 24.],\n",
       "        [24., 24.],\n",
       "        [40., 24.],\n",
       "        [56., 24.],\n",
       "        [ 8., 40.],\n",
       "        [24., 40.],\n",
       "        [40., 40.],\n",
       "        [56., 40.],\n",
       "        [ 8., 56.],\n",
       "        [24., 56.],\n",
       "        [40., 56.],\n",
       "        [56., 56.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def coords_fmap2orig(feature, stride=16):  # 特征图上的grid对应于原图的位置\n",
    "    '''\n",
    "    transfor one fmap coords to orig coords\n",
    "    Args:\n",
    "        featurn [batch_size,h,w,c]\n",
    "        stride int\n",
    "    Returns ：\n",
    "        coords [n,2]\n",
    "    '''\n",
    "    h, w = feature.shape[1:3]  \n",
    "    # h, w = 8, 8 # 为演示方便，我们使用尺寸为（4，4）的特征图\n",
    "    shifts_x = torch.arange(0, w * stride, stride, dtype=torch.float32)\n",
    "    shifts_y = torch.arange(0, h * stride, stride, dtype=torch.float32)\n",
    "    shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)\n",
    "    shift_x = torch.reshape(shift_x, [-1])\n",
    "    shift_y = torch.reshape(shift_y, [-1])\n",
    "    coords = torch.stack([shift_x, shift_y], -1) + (stride // 2)  # 中心点偏置\n",
    "    # 可视化一下，看看\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.scatter(coords[:,0], coords[:,1])\n",
    "    # plt.savefig('grid.png')\n",
    "    return coords\n",
    "\n",
    "coords = coords_fmap2orig(cls_logits)\n",
    "coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/perception field.png\" width=\"700\" height=\"400\" align=\"\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用形状转换，将特征图降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cls_logits \n",
    "cls_logits = cls_logits.reshape((batch_size, -1, class_num))  # [batch_size,h*w,class_num]\n",
    "cls_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/reshape1.png\" width=\"600\" height=\"400\" align=\"bottom\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 reshape cnt\n",
    "cnt_logits = torch.from_numpy(cnt_logits)\n",
    "cnt_logits = cnt_logits.permute(0, 2, 3, 1)   \n",
    "cnt_logits = cnt_logits.reshape((batch_size, -1, 1))    # [batch_size,h*w,1]\n",
    "cnt_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 reshape reg\n",
    "reg_preds = torch.from_numpy(reg_preds)\n",
    "reg_preds = reg_preds.permute(0, 2, 3, 1)\n",
    "reg_preds = reg_preds.reshape((batch_size, -1, 4))  #  # [batch_size,h*w,4]\n",
    "reg_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转换后特征图的形状特征\n",
    "h_mul_w = cls_logits.shape[1] \n",
    "h_mul_w  # h*w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 寻找特征图上的点与gtbox边缘的偏置关系\n",
    "\n",
    "&emsp;&emsp;特征图上feature_grid的坐标（x_fea,y_fea）先映射到原图上(x_img, y_img), 然后找点(x_img, y_img)与gtbox坐标(x_min,y_min, x_max,y_max)的偏置关系:当前点到框的各边的距离(l, t, r, b)\n",
    "\n",
    "<img src=\"imgs/tensor2fea2img.png\" width=\"900\" height=\"400\" align=\"bottom\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8., 24., 40., 56.,  8., 24., 40., 56.,  8., 24., 40., 56.,  8., 24.,\n",
      "        40., 56.])\n",
      "tensor([ 8.,  8.,  8.,  8., 24., 24., 24., 24., 40., 40., 40., 40., 56., 56.,\n",
      "        56., 56.])\n"
     ]
    }
   ],
   "source": [
    "x = coords[:, 0] ; print(x)\n",
    "y = coords[:, 1] ; print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gt_boxes = np.array([[[10, 12, 48, 60],\n",
    "#                       [10, 20, 40, 50]]]) \n",
    "gt_boxes[..., 0]  # 取第一列  所有gt框的xmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2., -2.],\n",
       "         [14., 14.],\n",
       "         [30., 30.],\n",
       "         [46., 46.],\n",
       "         [-2., -2.],\n",
       "         [14., 14.],\n",
       "         [30., 30.],\n",
       "         [46., 46.],\n",
       "         [-2., -2.],\n",
       "         [14., 14.],\n",
       "         [30., 30.],\n",
       "         [46., 46.],\n",
       "         [-2., -2.],\n",
       "         [14., 14.],\n",
       "         [30., 30.],\n",
       "         [46., 46.]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [1,h*w,1]-[batch_size,1,m]-->[batch_size,h*w,m]\n",
    "l_off = x[None, :, None] - gt_boxes[..., 0][:, None, :]   # None 增加维度用的\n",
    "l_off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/sub.png\" width=\"400\" height=\"400\" align=\"bottom\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "t_off = y[None, :, None] - gt_boxes[..., 1][:, None, :]\n",
    "\n",
    "r_off = gt_boxes[..., 2][:, None, :] - x[None, :, None]\n",
    "b_off = gt_boxes[..., 3][:, None, :] - y[None, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 2, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltrb_off = torch.stack([l_off, t_off, r_off, b_off], dim=-1)  # [batch_size,h*w,m,4]\n",
    "ltrb_off.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ -2.,  -4.,  40.,  52.],\n",
       "          [ -2., -12.,  32.,  42.]],\n",
       "\n",
       "         [[ 14.,  -4.,  24.,  52.],\n",
       "          [ 14., -12.,  16.,  42.]],\n",
       "\n",
       "         [[ 30.,  -4.,   8.,  52.],\n",
       "          [ 30., -12.,   0.,  42.]],\n",
       "\n",
       "         [[ 46.,  -4.,  -8.,  52.],\n",
       "          [ 46., -12., -16.,  42.]],\n",
       "\n",
       "         [[ -2.,  12.,  40.,  36.],\n",
       "          [ -2.,   4.,  32.,  26.]],\n",
       "\n",
       "         [[ 14.,  12.,  24.,  36.],\n",
       "          [ 14.,   4.,  16.,  26.]],\n",
       "\n",
       "         [[ 30.,  12.,   8.,  36.],\n",
       "          [ 30.,   4.,   0.,  26.]],\n",
       "\n",
       "         [[ 46.,  12.,  -8.,  36.],\n",
       "          [ 46.,   4., -16.,  26.]],\n",
       "\n",
       "         [[ -2.,  28.,  40.,  20.],\n",
       "          [ -2.,  20.,  32.,  10.]],\n",
       "\n",
       "         [[ 14.,  28.,  24.,  20.],\n",
       "          [ 14.,  20.,  16.,  10.]],\n",
       "\n",
       "         [[ 30.,  28.,   8.,  20.],\n",
       "          [ 30.,  20.,   0.,  10.]],\n",
       "\n",
       "         [[ 46.,  28.,  -8.,  20.],\n",
       "          [ 46.,  20., -16.,  10.]],\n",
       "\n",
       "         [[ -2.,  44.,  40.,   4.],\n",
       "          [ -2.,  36.,  32.,  -6.]],\n",
       "\n",
       "         [[ 14.,  44.,  24.,   4.],\n",
       "          [ 14.,  36.,  16.,  -6.]],\n",
       "\n",
       "         [[ 30.,  44.,   8.,   4.],\n",
       "          [ 30.,  36.,   0.,  -6.]],\n",
       "\n",
       "         [[ 46.,  44.,  -8.,   4.],\n",
       "          [ 46.,  36., -16.,  -6.]]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 16个(x_img,y_img)点与这个gt_box(x_min,y_min,x_max,y_max)的偏置关系\n",
    "ltrb_off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 选择feature map上的正负样本点\n",
    "&emsp;&emsp;对于feature map上的各个点（x_fea,y_fea），只要这个点对应的(x_img, y_img)落到了gt bbox(x_min,y_min, x_max,y_max)区域中，那么这个点就是正样本；而如果这个点多在多个bbox中，那个这个点就是模糊样本，目前采用面积最小的bbox作为这个点的回归目标。目标框的回归参数是：当前点到框的各边的距离(l, t, r, b)。\n",
    "<img src=\"imgs/ancher free.png\" width=\"500\" height=\"400\" align=\"bottom\">\n",
    ">```python\n",
    "l_off = x[None, :, None] - gt_boxes[..., 0][:, None, :]\n",
    "t_off = y[None, :, None] - gt_boxes[..., 1][:, None, :]\n",
    "r_off = gt_boxes[..., 2][:, None, :] - x[None, :, None]\n",
    "b_off = gt_boxes[..., 3][:, None, :] - y[None, :, None]\n",
    "ltrb_off = torch.stack([l_off, t_off, r_off, b_off], dim=-1)  # [batch_size,h*w,m,4]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.]]])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_min = torch.min(ltrb_off, dim=-1)[0]  # [batch_size,h*w,m]\n",
    "off_max = torch.max(ltrb_off, dim=-1)[0]  # [batch_size,h*w,m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [1, 1],\n",
       "         [1, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [1, 1],\n",
       "         [1, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [1, 0],\n",
       "         [1, 0],\n",
       "         [0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "off_min > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_in_gtboxes = off_min > 0  # 满足条件的为真 1 保留"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_in_level = (off_max > 32) & (off_max <= 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/scale.png\" width=\"600\" height=\"400\" align=\"bottom\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 寻找正样本点与gtbox中心的偏置关系\n",
    "<img src=\"imgs/ancher free.png\" width=\"500\" height=\"400\" align=\"bottom\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[29., 25.]])\n",
      "tensor([[36., 35.]])\n"
     ]
    }
   ],
   "source": [
    "# 计算gt_box的中心  我们假设有两个gt box \n",
    "gt_center_x = (gt_boxes[..., 0] + gt_boxes[..., 2]) / 2  ; print(gt_center_x)\n",
    "gt_center_y = (gt_boxes[..., 1] + gt_boxes[..., 3]) / 2 ; print(gt_center_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_box 中心点与 feat_map grid 间的偏置关系\n",
    "# | x - gt_center_x | x方向上的距离\n",
    "# | y - gt_center_y | y方向上的距离\n",
    "c_l_off = x[None, :, None] - gt_center_x[:, None, :]  # [1,h*w,1]-[batch_size,1,m]-->[batch_size,h*w,m]\n",
    "c_t_off = y[None, :, None] - gt_center_y[:, None, :]\n",
    "c_r_off = gt_center_x[:, None, :] - x[None, :, None]\n",
    "c_b_off = gt_center_y[:, None, :] - y[None, :, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_ltrb_off = torch.stack([c_l_off, c_t_off, c_r_off, c_b_off], dim=-1)  # [batch_size,h*w,m,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[28., 27.],\n",
       "         [28., 27.],\n",
       "         [28., 27.],\n",
       "         [28., 31.],\n",
       "         [21., 17.],\n",
       "         [12., 11.],\n",
       "         [12., 15.],\n",
       "         [27., 31.],\n",
       "         [21., 17.],\n",
       "         [ 5.,  5.],\n",
       "         [11., 15.],\n",
       "         [27., 31.],\n",
       "         [21., 21.],\n",
       "         [20., 21.],\n",
       "         [20., 21.],\n",
       "         [27., 31.]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_off_max = torch.max(c_ltrb_off, dim=-1)[0]  # 相对于gt_box center 偏的最远的grid\n",
    "c_off_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [0, 0],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [0, 0],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_center = c_off_max < 1.5 * 16  # 1 * stride / 1.5*stride\n",
    "mask_center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/center.png\" width=\"400\" height=\"400\" align=\"bottom\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/level_scale2.png\" width=\"400\" height=\"400\" align=\"bottom\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/level_scale1.png\" width=\"400\" height=\"400\" align=\"bottom\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [0, 0]]], dtype=torch.uint8) torch.Size([1, 16, 2])\n"
     ]
    }
   ],
   "source": [
    "mask_pos = mask_in_gtboxes & mask_in_level & mask_center \n",
    "print(mask_pos,mask_pos.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 根据gt_box面积分配模糊正样本点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于feature map上的各个点（x_fea,y_fea），只要这个点对应的(x_img, y_img)落到了gt bbox(x_min,y_min, x_max,y_max)区域中，那么这个点就是正样本；而如果这个点多在多个bbox中，那个这个点就是模糊样本，目前采用面积最小的bbox作为这个点的回归目标。目标框的回归方式也从回归顶点坐标(x, y, x, y)到回归当前点到框的各边的距离(l, t, r, b)。\n",
    "<img src=\"imgs/ancher free.png\" width=\"500\" height=\"400\" align=\"bottom\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.],\n",
      "         [1824.,  900.]]])\n"
     ]
    }
   ],
   "source": [
    "areas = (ltrb_off[..., 0] + ltrb_off[..., 2]) * (ltrb_off[..., 1] + ltrb_off[..., 3])  # [batch_size,h*w,m]\n",
    "print(areas)  # 框的面积值 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[99999., 99999.],\n",
       "         [99999., 99999.],\n",
       "         [99999., 99999.],\n",
       "         [99999., 99999.],\n",
       "         [99999., 99999.],\n",
       "         [ 1824., 99999.],\n",
       "         [ 1824., 99999.],\n",
       "         [99999., 99999.],\n",
       "         [99999., 99999.],\n",
       "         [99999., 99999.],\n",
       "         [99999., 99999.],\n",
       "         [99999., 99999.],\n",
       "         [99999., 99999.],\n",
       "         [ 1824., 99999.],\n",
       "         [ 1824., 99999.],\n",
       "         [99999., 99999.]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "areas[~mask_pos] = 99999 # 初始化非样本点 \n",
    "areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "areas_min_ind = torch.min(areas, dim=-1)[1]  # [batch_size,h*w]\n",
    "areas_min_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "areas_min_ind.unsqueeze(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros_like(areas, dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [1, 0],\n",
       "         [1, 0],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [1, 0],\n",
       "         [1, 0],\n",
       "         [0, 1]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成独热向量\n",
    "index = torch.zeros_like(areas, dtype=torch.uint8).scatter_(-1, \n",
    "                                                    areas_min_ind.unsqueeze(dim=-1),  # 增加一个维度\n",
    "                                                    1)\n",
    "print(index.shape)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 2, 4])\n",
      "torch.Size([16, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ -2., -12.,  32.,  42.],\n",
       "        [ 14., -12.,  16.,  42.],\n",
       "        [ 30., -12.,   0.,  42.],\n",
       "        [ 46., -12., -16.,  42.],\n",
       "        [ -2.,   4.,  32.,  26.],\n",
       "        [ 14.,  12.,  24.,  36.],\n",
       "        [ 30.,  12.,   8.,  36.],\n",
       "        [ 46.,   4., -16.,  26.],\n",
       "        [ -2.,  20.,  32.,  10.],\n",
       "        [ 14.,  20.,  16.,  10.],\n",
       "        [ 30.,  20.,   0.,  10.],\n",
       "        [ 46.,  20., -16.,  10.],\n",
       "        [ -2.,  36.,  32.,  -6.],\n",
       "        [ 14.,  44.,  24.,   4.],\n",
       "        [ 30.,  44.,   8.,   4.],\n",
       "        [ 46.,  36., -16.,  -6.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定一下面积最小的那个框，特征图上的每个格点对应到它所属的面积最小的gt_box\n",
    "print(ltrb_off.shape)\n",
    "reg_targets = ltrb_off[index]  # [batch_size*h*w, 4]\n",
    "print(reg_targets.shape)\n",
    "reg_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = torch.broadcast_tensors(classes[:, None, :], areas.long())[0] # areas.long() 转换area的数值类型\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 4, 4, 2, 2, 4], dtype=torch.int32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select = torch.zeros_like(areas, dtype=torch.uint8).scatter_(-1, areas_min_ind.unsqueeze(dim=-1), 1)\n",
    "cls_targets = classes[select]\n",
    "cls_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [cnt_targets 在做什么?](https://blog.csdn.net/shanglianlm/article/details/89007219)\n",
    "**centerness**\n",
    "\n",
    "&emsp;&emsp;通过center-ness来度量当前位置和物体中心间的距离，即FCOS将点的坐标在目标中的位置因素也考虑进来，越靠近中间权重越大。\n",
    "<img src=\"imgs/centerness.jpg\" width=\"500\" height=\"400\" align=\"bottom\">\n",
    "\n",
    "\n",
    "&emsp;&emsp;在训练的过程中通过损失函数，我们会约束center-ness的值，使得其接近于0，使得分布在目标位置边缘的低质量框能够尽可能的靠近中心。在最终使用该网络的过程中，非极大值抑制(NMS)就可以轻松滤除这些低质量的边界框，提高检测性能。\n",
    "<img src=\"imgs/equation3.png\" width=\"500\" height=\"400\" align=\"bottom\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -2.,  14.,   0., -16.,  -2.,  14.,   8., -16.,  -2.,  14.,   0., -16.,\n",
      "         -2.,  14.,   8., -16.])\n",
      "tensor([32., 16., 30., 46., 32., 24., 30., 46., 32., 16., 30., 46., 32., 24.,\n",
      "        30., 46.])\n"
     ]
    }
   ],
   "source": [
    "left_right_min = torch.min(reg_targets[..., 0], reg_targets[..., 2]) ; print(left_right_min)\n",
    "left_right_max = torch.max(reg_targets[..., 0], reg_targets[..., 2]) ; print(left_right_max)\n",
    "top_bottom_min = torch.min(reg_targets[..., 1], reg_targets[..., 3])\n",
    "top_bottom_max = torch.max(reg_targets[..., 1], reg_targets[..., 3])\n",
    "# 上面的值拿来计算中心目标\n",
    "cnt_targets = ((left_right_min * top_bottom_min) / (left_right_max * top_bottom_max + 1e-4)).sqrt().unsqueeze(\n",
    "    dim=-1)  # [batch_size,h*w,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1336],\n",
       "        [   nan],\n",
       "        [-0.0000],\n",
       "        [0.3152],\n",
       "        [   nan],\n",
       "        [0.4410],\n",
       "        [0.2981],\n",
       "        [   nan],\n",
       "        [   nan],\n",
       "        [0.6614],\n",
       "        [0.0000],\n",
       "        [   nan],\n",
       "        [0.1021],\n",
       "        [0.2303],\n",
       "        [0.1557],\n",
       "        [0.2408]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 根据mask制作taragets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask_pos = mask_in_gtboxes & mask_in_level & mask_center \n",
    "mask_pos.long().sum(dim=-1)  # [batch_size,h*w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_pos_2 = mask_pos.long().sum(dim=-1)  # [batch_size,h*w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_pos_2 = mask_pos_2 >= 1\n",
    "mask_pos_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 4, 4, 2, 2, 4], dtype=torch.int32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0], dtype=torch.int32)\n",
      "tensor([[-1.0000],\n",
      "        [-1.0000],\n",
      "        [-1.0000],\n",
      "        [-1.0000],\n",
      "        [-1.0000],\n",
      "        [ 0.4410],\n",
      "        [ 0.2981],\n",
      "        [-1.0000],\n",
      "        [-1.0000],\n",
      "        [-1.0000],\n",
      "        [-1.0000],\n",
      "        [-1.0000],\n",
      "        [-1.0000],\n",
      "        [ 0.2303],\n",
      "        [ 0.1557],\n",
      "        [-1.0000]])\n",
      "tensor([[-1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [14., 12., 24., 36.],\n",
      "        [30., 12.,  8., 36.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [14., 44., 24.,  4.],\n",
      "        [30., 44.,  8.,  4.],\n",
      "        [-1., -1., -1., -1.]])\n"
     ]
    }
   ],
   "source": [
    "# 对于只有一个gt_box的情况\n",
    "cls_targets[~mask_pos_2.squeeze()] = 0   # [batch_size,h*w,1]\n",
    "cnt_targets[~mask_pos_2.squeeze()] = -1  \n",
    "reg_targets[~mask_pos_2.squeeze()] = -1\n",
    "\n",
    "print(cls_targets)\n",
    "print(cnt_targets)\n",
    "print(reg_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总体函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "  def _gen_level_targets(out, gt_boxes, classes, stride, limit_range, sample_radiu_ratio=1.5):\n",
    "        '''\n",
    "        Args  \n",
    "        out list contains [[batch_size,class_num,h,w],[batch_size,1,h,w],[batch_size,4,h,w]]  \n",
    "        gt_boxes [batch_size,m,4]  \n",
    "        classes [batch_size,m]  \n",
    "        stride int  \n",
    "        limit_range list [min,max]  \n",
    "        Returns  \n",
    "        cls_targets,cnt_targets,reg_targets\n",
    "        '''\n",
    "        # 1 三个同一层的分类 中心 定位的 预测数据\n",
    "        cls_logits, cnt_logits, reg_preds = out\n",
    "        batch_size = cls_logits.shape[0]\n",
    "        class_num = cls_logits.shape[1]\n",
    "        # 2 一个图片有多少个标签框\n",
    "        m = gt_boxes.shape[1]\n",
    "        # 3 reshape (batch_size, h*w, n)\n",
    "        cls_logits = cls_logits.permute(0, 2, 3, 1)  # [batch_size,h,w,class_num]\n",
    "        coords = coords_fmap2orig(cls_logits, stride).to(device=gt_boxes.device)  # [h*w,2]\n",
    "\n",
    "        cls_logits = cls_logits.reshape((batch_size, -1, class_num))  # [batch_size,h*w,class_num]\n",
    "        # 4 reshape cnt\n",
    "        cnt_logits = cnt_logits.permute(0, 2, 3, 1)   \n",
    "        cnt_logits = cnt_logits.reshape((batch_size, -1, 1))    # [batch_size,h*w,1]\n",
    "        # 5 reshape reg\n",
    "        reg_preds = reg_preds.permute(0, 2, 3, 1)\n",
    "        reg_preds = reg_preds.reshape((batch_size, -1, 4))  #  # [batch_size,h*w,4]\n",
    "\n",
    "        h_mul_w = cls_logits.shape[1]  # h*w\n",
    "\n",
    "        x = coords[:, 0]\n",
    "        y = coords[:, 1]\n",
    "        # 6 判断gt_box 包含了特征图上哪些grid(格点)\n",
    "        l_off = x[None, :, None] - gt_boxes[..., 0][:, None, :]  # [1,h*w,1]-[batch_size,1,m]-->[batch_size,h*w,m]\n",
    "        t_off = y[None, :, None] - gt_boxes[..., 1][:, None, :]\n",
    "        \n",
    "        r_off = gt_boxes[..., 2][:, None, :] - x[None, :, None]\n",
    "        b_off = gt_boxes[..., 3][:, None, :] - y[None, :, None]\n",
    "        \n",
    "        ltrb_off = torch.stack([l_off, t_off, r_off, b_off], dim=-1)  # [batch_size,h*w,m,4]\n",
    "\n",
    "        areas = (ltrb_off[..., 0] + ltrb_off[..., 2]) * (ltrb_off[..., 1] + ltrb_off[..., 3])  # [batch_size,h*w,m]\n",
    "\n",
    "        \n",
    "        off_min = torch.min(ltrb_off, dim=-1)[0]  # [batch_size,h*w,m]\n",
    "        off_max = torch.max(ltrb_off, dim=-1)[0]  # [batch_size,h*w,m]\n",
    "\n",
    "        mask_in_gtboxes = off_min > 0  # feature map 上的gird 是不是在框里\n",
    "        mask_in_level = (off_max > limit_range[0]) & (off_max <= limit_range[1])  # 这个框是不是在这一层里\n",
    "        # 7 找到离gt box 不远（1*stride）的grid\n",
    "        radiu = stride * sample_radiu_ratio\n",
    "        gt_center_x = (gt_boxes[..., 0] + gt_boxes[..., 2]) / 2\n",
    "        gt_center_y = (gt_boxes[..., 1] + gt_boxes[..., 3]) / 2\n",
    "        \n",
    "        c_l_off = x[None, :, None] - gt_center_x[:, None, :]  # [1,h*w,1]-[batch_size,1,m]-->[batch_size,h*w,m]\n",
    "        c_t_off = y[None, :, None] - gt_center_y[:, None, :]\n",
    "        c_r_off = gt_center_x[:, None, :] - x[None, :, None]\n",
    "        c_b_off = gt_center_y[:, None, :] - y[None, :, None]\n",
    "        \n",
    "        c_ltrb_off = torch.stack([c_l_off, c_t_off, c_r_off, c_b_off], dim=-1)  # [batch_size,h*w,m,4]\n",
    "        c_off_max = torch.max(c_ltrb_off, dim=-1)[0]\n",
    "        mask_center = c_off_max < radiu\n",
    "        # 选择 满足条件的grid\n",
    "        mask_pos = mask_in_gtboxes & mask_in_level & mask_center  # [batch_size,h*w,m]\n",
    "\n",
    "        areas[~mask_pos] = 99999999\n",
    "        areas_min_ind = torch.min(areas, dim=-1)[1]  # [batch_size,h*w]\n",
    "        reg_targets = ltrb_off[torch.zeros_like(areas, dtype=torch.uint8).scatter_(-1, areas_min_ind.unsqueeze(dim=-1),\n",
    "                                                                                   1)]  # [batch_size*h*w,4]\n",
    "        reg_targets = torch.reshape(reg_targets, (batch_size, -1, 4))  # [batch_size,h*w,4]\n",
    "\n",
    "        classes = torch.broadcast_tensors(classes[:, None, :], areas.long())[0]  # [batch_size,h*w,m]\n",
    "        cls_targets = classes[\n",
    "            torch.zeros_like(areas, dtype=torch.uint8).scatter_(-1, areas_min_ind.unsqueeze(dim=-1), 1)]\n",
    "        cls_targets = torch.reshape(cls_targets, (batch_size, -1, 1))  # [batch_size,h*w,1]\n",
    "\n",
    "        left_right_min = torch.min(reg_targets[..., 0], reg_targets[..., 2])  # [batch_size,h*w]\n",
    "        left_right_max = torch.max(reg_targets[..., 0], reg_targets[..., 2])\n",
    "        top_bottom_min = torch.min(reg_targets[..., 1], reg_targets[..., 3])\n",
    "        top_bottom_max = torch.max(reg_targets[..., 1], reg_targets[..., 3])\n",
    "        cnt_targets = ((left_right_min * top_bottom_min) / (left_right_max * top_bottom_max + 1e-10)).sqrt().unsqueeze(\n",
    "            dim=-1)  # [batch_size,h*w,1]\n",
    "\n",
    "        assert reg_targets.shape == (batch_size, h_mul_w, 4)\n",
    "        assert cls_targets.shape == (batch_size, h_mul_w, 1)\n",
    "        assert cnt_targets.shape == (batch_size, h_mul_w, 1)\n",
    "\n",
    "        # process neg coords\n",
    "        mask_pos_2 = mask_pos.long().sum(dim=-1)  # [batch_size,h*w]\n",
    "        # num_pos=mask_pos_2.sum(dim=-1)\n",
    "        # assert num_pos.shape==(batch_size,)\n",
    "        mask_pos_2 = mask_pos_2 >= 1  # 再确认一下\n",
    "        assert mask_pos_2.shape == (batch_size, h_mul_w)\n",
    "        cls_targets[~mask_pos_2] = 0   # [batch_size,h*w,1]\n",
    "        cnt_targets[~mask_pos_2] = -1  \n",
    "        reg_targets[~mask_pos_2] = -1\n",
    "\n",
    "        return cls_targets, cnt_targets, reg_targets"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "torch1.1",
   "language": "python",
   "name": "torch1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "233.83px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
