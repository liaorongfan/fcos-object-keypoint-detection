{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/loss.png\" width=\"900\" height=\"400\" align=\"bottom\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class DefaultConfig():\n",
    "    # backbone\n",
    "    backbone = \"darknet19\"\n",
    "    pretrained = True\n",
    "    freeze_stage_1 = True\n",
    "    freeze_bn = True\n",
    "\n",
    "    # fpn\n",
    "    fpn_out_channels = 256\n",
    "    use_p5 = True\n",
    "    \n",
    "    # head\n",
    "    class_num = 5\n",
    "    use_GN_head = True\n",
    "    prior = 0.01\n",
    "    add_centerness = True\n",
    "    cnt_on_reg = False\n",
    "\n",
    "    # training\n",
    "    strides = [8, 16, 32, 64, 128]\n",
    "    limit_range = [[-1, 64], [64, 128], [128, 256], [256, 512], [512, 999999]]\n",
    "\n",
    "    # inference\n",
    "    score_threshold = 0.3\n",
    "    nms_iou_threshold = 0.2\n",
    "    max_detection_boxes_num = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class LOSS(nn.Module):\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "        if config is None:\n",
    "            self.config = DefaultConfig\n",
    "        else:\n",
    "            self.config = config\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs list\n",
    "        [0]preds:  ....\n",
    "        [1]targets : list contains three elements [[batch_size,sum(_h*_w),1],[batch_size,sum(_h*_w),1],[batch_size,sum(_h*_w),4]]\n",
    "        \"\"\"\n",
    "        preds, targets = inputs\n",
    "        cls_logits, cnt_logits, reg_preds = preds\n",
    "        cls_targets, cnt_targets, reg_targets = targets\n",
    "        \n",
    "        mask_pos = (cnt_targets > -1).squeeze(dim=-1)  # [batch_size,sum(_h*_w)]\n",
    "        cls_loss = compute_cls_loss(cls_logits, cls_targets, mask_pos).mean()  # []\n",
    "        cnt_loss = compute_cnt_loss(cnt_logits, cnt_targets, mask_pos).mean()\n",
    "        reg_loss = compute_reg_loss(reg_preds, reg_targets, mask_pos).mean()\n",
    "        \n",
    "        if self.config.add_centerness:\n",
    "            total_loss = cls_loss + cnt_loss + reg_loss\n",
    "            return cls_loss, cnt_loss, reg_loss, total_loss\n",
    "        else:\n",
    "            total_loss = cls_loss + reg_loss + cnt_loss * 0.0\n",
    "            return cls_loss, cnt_loss, reg_loss, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def focal_loss_from_logits(preds, targets, gamma=2.0, alpha=0.25):\n",
    "    '''\n",
    "    Args:\n",
    "    preds: [n,class_num] \n",
    "    targets: [n,class_num]\n",
    "    '''\n",
    "    preds = preds.sigmoid()\n",
    "    pt = preds * targets + (1.0 - preds) * (1.0 - targets)\n",
    "    w = alpha * targets + (1.0 - alpha) * (1.0 - targets)\n",
    "    loss = -w * torch.pow((1.0 - pt), gamma) * pt.log()\n",
    "    return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def compute_cls_loss(preds, targets, mask):\n",
    "    '''\n",
    "    Args  \n",
    "    preds: list contains five level pred [batch_size,class_num,_h,_w]\n",
    "    targets: [batch_size,sum(_h*_w),1]\n",
    "    mask: [batch_size,sum(_h*_w)]\n",
    "    '''\n",
    "    batch_size = targets.shape[0]\n",
    "    \n",
    "    preds_reshape = []\n",
    "    class_num = preds[0].shape[1]  # channels \n",
    "    mask = mask.unsqueeze(dim=-1)  # unsqueeze() 添加列维度\n",
    "    # mask=targets>-1#[batch_size,sum(_h*_w),1]\n",
    "    \n",
    "    num_pos = torch.sum(mask, dim=[1, 2]).clamp_(min=1).float()  # [batch_size,]\n",
    "    \n",
    "    for pred in preds:\n",
    "        pred = pred.permute(0, 2, 3, 1)  #  （bs, c, h, w) --> (bs, h, w, c)\n",
    "        pred = torch.reshape(pred, [batch_size, -1, class_num])  # --> (bs, h*w, c)\n",
    "        preds_reshape.append(pred)\n",
    "    preds = torch.cat(preds_reshape, dim=1)  # [bs,(h1*w1 + h2*w2 + ...),class_num]\n",
    "    \n",
    "    assert preds.shape[:2] == targets.shape[:2]\n",
    "    loss = []\n",
    "    for batch_index in range(batch_size):\n",
    "        pred_pos = preds[batch_index]  # [sum(_h*_w),class_num]\n",
    "        target_pos = targets[batch_index]  # [sum(_h*_w),1]\n",
    "        target_pos = (\n",
    "            torch.arange(1, class_num + 1, device=target_pos.device)[None,:].type(torch.float32) == target_pos\n",
    "        ).float()  # sparse-->onehot\n",
    "        loss.append(focal_loss_from_logits(pred_pos, target_pos).view(1))\n",
    "    return torch.cat(loss, dim=0) / num_pos  # [batch_size,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.1113, 2.1113])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [torch.ones([2, 5, 4, 4])] * 5  # five (5, 4, 4) feature maps with batch size of 2\n",
    "targets = torch.ones([2, 80, 1])\n",
    "targets[0, 1, 0] = 3\n",
    "targets[0, 2, 0] = 4\n",
    "targets[0, 3, 0] = 2\n",
    "mask = torch.ones([2, 80], dtype=torch.uint8)\n",
    "compute_cls_loss(preds, targets, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = targets.shape[0]  # set to 2\n",
    "class_num = preds[0].shape[1]  # set to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.ones([2, 80], dtype=torch.uint8)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 80, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = mask.unsqueeze(dim=-1)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([80., 80.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_pos = torch.sum(mask, dim=[1, 2]).clamp_(min=1).float()  # [batch_size,]  # _ 表示 inplace操作\n",
    "num_pos   # added entries in axis=1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 4, 5])\n",
      "torch.Size([2, 16, 5])\n",
      "torch.Size([2, 4, 4, 5])\n",
      "torch.Size([2, 16, 5])\n",
      "torch.Size([2, 4, 4, 5])\n",
      "torch.Size([2, 16, 5])\n",
      "torch.Size([2, 4, 4, 5])\n",
      "torch.Size([2, 16, 5])\n",
      "torch.Size([2, 4, 4, 5])\n",
      "torch.Size([2, 16, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 80, 5])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_reshape = []\n",
    "for pred in preds:\n",
    "    pred = pred.permute(0, 2, 3, 1)\n",
    "    print(pred.shape)\n",
    "    pred = torch.reshape(pred, [batch_size, -1, class_num])\n",
    "    print(pred.shape)\n",
    "    preds_reshape.append(pred)\n",
    "        \n",
    "preds = torch.cat(preds_reshape, dim=1)  # [batch_size,sum(_h*_w),class_num]\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert preds.shape[:2] == targets.shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.arange(1, class_num + 1, device=target_pos.device)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[None,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 5])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_pos = (tensor[None,:].type(torch.float32) == target_pos).float()  # sparse-->onehot\n",
    "target_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.5653])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focal_loss_from_logits(pred_pos, target_pos).view(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中心度损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def compute_cnt_loss(preds, targets, mask):\n",
    "    '''\n",
    "    Args  \n",
    "    preds: list contains five level pred [batch_size,1,_h,_w]\n",
    "    targets: [batch_size,sum(_h*_w),1]\n",
    "    mask: [batch_size,sum(_h*_w)]\n",
    "    '''\n",
    "    batch_size = targets.shape[0]\n",
    "    c = targets.shape[-1]\n",
    "    preds_reshape = []\n",
    "    mask = mask.unsqueeze(dim=-1)\n",
    "    # mask=targets>-1#[batch_size,sum(_h*_w),1]\n",
    "    num_pos = torch.sum(mask, dim=[1, 2]).clamp_(min=1).float()  # [batch_size,]\n",
    "    for pred in preds:\n",
    "        pred = pred.permute(0, 2, 3, 1)\n",
    "        pred = torch.reshape(pred, [batch_size, -1, c])\n",
    "        preds_reshape.append(pred)\n",
    "    preds = torch.cat(preds_reshape, dim=1)\n",
    "    assert preds.shape == targets.shape  # [batch_size,sum(_h*_w),1]\n",
    "    loss = []\n",
    "    for batch_index in range(batch_size):\n",
    "        pred_pos = preds[batch_index][mask[batch_index]]  # [num_pos_b,]\n",
    "        target_pos = targets[batch_index][mask[batch_index]]  # [num_pos_b,]\n",
    "        assert len(pred_pos.shape) == 1\n",
    "        loss.append(\n",
    "            nn.functional.binary_cross_entropy_with_logits(input=pred_pos, target=target_pos, reduction='sum').view(1))\n",
    "    return torch.cat(loss, dim=0) / num_pos  # [batch_size,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3133, 0.3133])\n"
     ]
    }
   ],
   "source": [
    "loss = compute_cnt_loss([torch.ones([2, 1, 4, 4])] * 5, torch.ones([2, 80, 1]),\n",
    "                        torch.ones([2, 80], dtype=torch.uint8))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定位损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def compute_reg_loss(preds, targets, mask, mode='iou'):\n",
    "    '''\n",
    "    Args  \n",
    "    preds: list contains five level pred [batch_size,4,_h,_w]\n",
    "    targets: [batch_size,sum(_h*_w),4]\n",
    "    mask: [batch_size,sum(_h*_w)]\n",
    "    '''\n",
    "    batch_size = targets.shape[0]\n",
    "    c = targets.shape[-1]\n",
    "    preds_reshape = []\n",
    "    # mask=targets>-1#[batch_size,sum(_h*_w),4]\n",
    "    num_pos = torch.sum(mask, dim=1).clamp_(min=1).float()  # [batch_size,]\n",
    "    for pred in preds:\n",
    "        pred = pred.permute(0, 2, 3, 1)\n",
    "        pred = torch.reshape(pred, [batch_size, -1, c])\n",
    "        preds_reshape.append(pred)\n",
    "    preds = torch.cat(preds_reshape, dim=1)\n",
    "    assert preds.shape == targets.shape  # [batch_size,sum(_h*_w),4]\n",
    "    loss = []\n",
    "    for batch_index in range(batch_size):\n",
    "        pred_pos = preds[batch_index][mask[batch_index]]  # [num_pos_b,4]\n",
    "        target_pos = targets[batch_index][mask[batch_index]]  # [num_pos_b,4]\n",
    "        assert len(pred_pos.shape) == 2\n",
    "        if mode == 'iou':\n",
    "            loss.append(iou_loss(pred_pos, target_pos).view(1))\n",
    "        elif mode == 'giou':\n",
    "            loss.append(giou_loss(pred_pos, target_pos).view(1))\n",
    "        else:\n",
    "            raise NotImplementedError(\"reg loss only implemented ['iou','giou']\")\n",
    "    return torch.cat(loss, dim=0) / num_pos  # [batch_size,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def iou_loss(preds, targets):\n",
    "    '''\n",
    "    Args:\n",
    "    preds: [n,4] ltrb\n",
    "    targets: [n,4]\n",
    "    '''\n",
    "    lt = torch.min(preds[:, :2], targets[:, :2])\n",
    "    rb = torch.min(preds[:, 2:], targets[:, 2:])\n",
    "    wh = (rb + lt).clamp(min=0)\n",
    "    overlap = wh[:, 0] * wh[:, 1]  # [n]\n",
    "    area1 = (preds[:, 2] + preds[:, 0]) * (preds[:, 3] + preds[:, 1])\n",
    "    area2 = (targets[:, 2] + targets[:, 0]) * (targets[:, 3] + targets[:, 1])\n",
    "    iou = overlap / (area1 + area2 - overlap)\n",
    "    loss = -iou.clamp(min=1e-6).log()\n",
    "    return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def giou_loss(preds, targets):\n",
    "    '''\n",
    "    Args:\n",
    "    preds: [n,4] ltrb\n",
    "    targets: [n,4]\n",
    "    '''\n",
    "    lt_min = torch.min(preds[:, :2], targets[:, :2])\n",
    "    rb_min = torch.min(preds[:, 2:], targets[:, 2:])\n",
    "    wh_min = (rb_min + lt_min).clamp(min=0)\n",
    "    overlap = wh_min[:, 0] * wh_min[:, 1]  # [n]\n",
    "    area1 = (preds[:, 2] + preds[:, 0]) * (preds[:, 3] + preds[:, 1])\n",
    "    area2 = (targets[:, 2] + targets[:, 0]) * (targets[:, 3] + targets[:, 1])\n",
    "    union = (area1 + area2 - overlap)\n",
    "    iou = overlap / union\n",
    "\n",
    "    lt_max = torch.max(preds[:, :2], targets[:, :2])\n",
    "    rb_max = torch.max(preds[:, 2:], targets[:, 2:])\n",
    "    wh_max = (rb_max + lt_max).clamp(0)\n",
    "    G_area = wh_max[:, 0] * wh_max[:, 1]  # [n]\n",
    "\n",
    "    giou = iou - (G_area - union) / G_area.clamp(1e-10)\n",
    "    loss = 1. - giou\n",
    "    return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "loss = compute_reg_loss([torch.ones([2, 4, 4, 4])] * 5, torch.ones([2, 80, 4]),\n",
    "                        torch.ones([2, 80], dtype=torch.uint8))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [推荐阅读](https://blog.csdn.net/qq_41917697/article/details/115372154)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "torch1.1",
   "language": "python",
   "name": "torch1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "170.273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
